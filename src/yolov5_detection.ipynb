{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "786653b2",
   "metadata": {},
   "source": [
    "This notebook allows to install YOLOv5 requirements, reformat training and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec5de422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import glob\n",
    "import shutil\n",
    "import sys\n",
    "import numpy as np\n",
    "import imagesize\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List\n",
    "import torch\n",
    "from torchvision.ops import box_iou\n",
    "from typing import List\n",
    "import torch\n",
    "from torchvision.ops import box_iou\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e7dd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "795cc4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/ultralytics/yolov5\n",
    "# %cd yolov5\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "# Train YOLOv5s on COCO128 for 1000 epochs using this command line \n",
    "# $ python train.py --img 640 --batch 16 --epochs 1000 --data starfish.yaml --weights yolov5s.pt\n",
    "# starfish.yaml -> path to train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d78f879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/peter/.cache/torch/hub/ultralytics_yolov5_master\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "YOLOv5 ðŸš€ 2022-4-11 torch 1.10.1+cu102 CUDA:0 (GeForce GTX 1080, 8112MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.5 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', autoshape=False) #autoshape=False for post-training with 1 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4302d546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check thatg everything is working \n",
    "\n",
    "# for f in 'zidane.jpg', 'bus.jpg':\n",
    "#     torch.hub.download_url_to_file('https://ultralytics.com/images/' + f, f)  # download 2 images\n",
    "# im1 = Image.open('zidane.jpg')  # PIL image\n",
    "# im2 = cv2.imread('bus.jpg')[..., ::-1]  # OpenCV image (BGR to RGB)\n",
    "# imgs = [im1, im2]  # batch of images\n",
    "\n",
    "# # Inference\n",
    "# results = model(imgs, size=640)  # includes NMS\n",
    "\n",
    "# # Results\n",
    "# results.print()  \n",
    "# results.save()  # or .show()\n",
    "\n",
    "# results.xyxy[0]  # im1 predictions (tensor)\n",
    "# results.pandas().xyxy[0]  # im1 predictions (pandas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af8c33ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR  = '/media/peter/2TB/julien/mlproject/great-barrier-reef'\n",
    "WORKING_DIR  = '/media/peter/2TB/julien/mlproject/great-barrier-reef'\n",
    "def get_path(row):\n",
    "    row['image_path'] = f'{ROOT_DIR}/train_images/video_{row.video_id}/{row.video_frame}.jpg'\n",
    "    row['label_path'] = f'{ROOT_DIR}/datasets/labels/video_{row.video_id}_{row.video_frame}.txt'\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bc89302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>video_frame</th>\n",
       "      <th>sequence_frame</th>\n",
       "      <th>image_id</th>\n",
       "      <th>annotations</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0-0</td>\n",
       "      <td>[]</td>\n",
       "      <td>/media/peter/2TB/julien/mlproject/great-barrie...</td>\n",
       "      <td>/media/peter/2TB/julien/mlproject/great-barrie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0-1</td>\n",
       "      <td>[]</td>\n",
       "      <td>/media/peter/2TB/julien/mlproject/great-barrie...</td>\n",
       "      <td>/media/peter/2TB/julien/mlproject/great-barrie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   video_id  sequence  video_frame  sequence_frame image_id annotations  \\\n",
       "0         0     40258            0               0      0-0          []   \n",
       "1         0     40258            1               1      0-1          []   \n",
       "\n",
       "                                          image_path  \\\n",
       "0  /media/peter/2TB/julien/mlproject/great-barrie...   \n",
       "1  /media/peter/2TB/julien/mlproject/great-barrie...   \n",
       "\n",
       "                                          label_path  \n",
       "0  /media/peter/2TB/julien/mlproject/great-barrie...  \n",
       "1  /media/peter/2TB/julien/mlproject/great-barrie...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(f'{ROOT_DIR}/train.csv')\n",
    "df = df.apply(get_path, axis=1)\n",
    "df['annotations'] = df['annotations'].apply(lambda x: ast.literal_eval(x))\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08bbd37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23501, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d34105ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% images without annotations: 79.06897578826432\n",
      "% images with  annotations: 20.93102421173567 \n"
     ]
    }
   ],
   "source": [
    "df['num_bbox'] = df['annotations'].apply(lambda x: len(x))\n",
    "data = (df.num_bbox>0).value_counts()/len(df)*100\n",
    "print('% images without annotations: {}'.format(data[0]))\n",
    "print('% images with  annotations: {} '.format(data[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f3a1fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.query(\"num_bbox>0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74c6c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COnverting annotations from coco to yolo\n",
    "def coco2yolo(image_height, image_width, bboxes):\n",
    "    \"\"\"\n",
    "    coco => [xmin, ymin, w, h]\n",
    "    yolo => [xmid, ymid, w, h] (normalized)\n",
    "    \"\"\"\n",
    "    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n",
    "    \n",
    "    # normalizinig\n",
    "    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]/ image_width\n",
    "    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]/ image_height\n",
    "    \n",
    "    # conversion (xmin, ymin) => (xmid, ymid)\n",
    "    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]/2\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "def yolo2coco(image_height, image_width, bboxes):\n",
    "    \"\"\"\n",
    "    yolo => [xmid, ymid, w, h] (normalized)\n",
    "    coco => [xmin, ymin, w, h]\n",
    "    \n",
    "    \"\"\" \n",
    "    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n",
    "    \n",
    "    # denormalizing\n",
    "    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]* image_width\n",
    "    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]* image_height\n",
    "    \n",
    "    # converstion (xmid, ymid) => (xmin, ymin) \n",
    "    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "def load_image(image_path):\n",
    "    return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "def get_bbox(annots):\n",
    "    bboxes = [list(annot.values()) for annot in annots]\n",
    "    return bboxes\n",
    "\n",
    "def get_imgsize(row):\n",
    "    row['width'], row['height'] = imagesize.get(row['image_path'])\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c20c93f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'x': 276, 'y': 631, 'width': 116, 'height': 88}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.iloc[100]['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a48c699b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1280])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([720])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>video_frame</th>\n",
       "      <th>sequence_frame</th>\n",
       "      <th>image_id</th>\n",
       "      <th>...</th>\n",
       "      <th>label_path</th>\n",
       "      <th>num_bbox</th>\n",
       "      <th>bboxes</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0-16</td>\n",
       "      <td>...</td>\n",
       "      <td>/media/peter/2TB/julien/mlproject/great-barrie...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[559, 213, 50, 32]]</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>0-17</td>\n",
       "      <td>...</td>\n",
       "      <td>/media/peter/2TB/julien/mlproject/great-barrie...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[558, 213, 50, 32]]</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    video_id  sequence  video_frame  sequence_frame image_id  ...  \\\n",
       "16         0     40258           16              16     0-16  ...   \n",
       "17         0     40258           17              17     0-17  ...   \n",
       "\n",
       "                                           label_path num_bbox  \\\n",
       "16  /media/peter/2TB/julien/mlproject/great-barrie...        1   \n",
       "17  /media/peter/2TB/julien/mlproject/great-barrie...        1   \n",
       "\n",
       "                  bboxes  width height  \n",
       "16  [[559, 213, 50, 32]]   1280    720  \n",
       "17  [[558, 213, 50, 32]]   1280    720  \n",
       "\n",
       "[2 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['bboxes'] = df.annotations.apply(get_bbox)\n",
    "df = df.apply(get_imgsize,axis=1)\n",
    "display(df.width.unique(), df.height.unique())\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45a776b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1100\n",
       "3     970\n",
       "2     968\n",
       "4     945\n",
       "1     936\n",
       "Name: fold, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from sklearn.model_selection import GroupKFold\n",
    "# kf = GroupKFold(n_splits = 5) \n",
    "# df = df.reset_index(drop=True)\n",
    "# df['fold'] = -1\n",
    "# for fold, (train_idx, val_idx) in enumerate(kf.split(df, y = df.video_id.tolist(), groups=df.sequence)):\n",
    "#     df.loc[val_idx, 'fold'] = fold\n",
    "# display(df.fold.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1e150b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3951, 13)\n",
      "(968, 13)\n"
     ]
    }
   ],
   "source": [
    "# val_df = df[df['fold']==2]\n",
    "# train_df = df[df['fold']!=2]\n",
    "# print(train_df.shape)\n",
    "# print(val_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e932c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11bf7af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_video_0 = 0\n",
    "n_video_1 = 1\n",
    "n_video_2 = 2\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    if df['video_id'][i] == 0:\n",
    "        n_video_0+= 1\n",
    "    if df['video_id'][i] == 1:\n",
    "        n_video_1+= 1\n",
    "    if df['video_id'][i] == 2:\n",
    "        n_video_2+= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98b5362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = 0\n",
    "for i in range(df.shape[0]):\n",
    "    if df.loc[i, 'video_id'] == 0:\n",
    "        cmp+= 1\n",
    "        if cmp > n_video_0*0.8:\n",
    "            df.loc[i, 'fold'] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "416194e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = 0\n",
    "for i in range(df.shape[0]):\n",
    "    if df.loc[i, 'video_id'] == 1:\n",
    "        cmp+= 1\n",
    "        if cmp > n_video_1*0.8:\n",
    "            df.loc[i, 'fold'] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07d2053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = 0\n",
    "for i in range(df.shape[0]):\n",
    "    if df.loc[i, 'video_id'] == 2:\n",
    "        cmp+= 1\n",
    "        if cmp > n_video_2*0.8:\n",
    "            df.loc[i, 'fold'] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e422b109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/peter/2TB/julien/mlproject/great-barrier-reef/datasets/training_data/labels/video_0_5048.txt'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_df.iloc[row_idx].label_path[:62] + 'training_data' + train_df.iloc[row_idx].label_path[61:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21ee51a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write labels in correct format\n",
    "\n",
    "cnt = 0\n",
    "for row_idx in (range(train_df.shape[0])):\n",
    "    row = train_df.iloc[row_idx]\n",
    "    image_height = row.height\n",
    "    image_width = row.width\n",
    "    bboxes_coco = np.asarray(row.bboxes).astype(np.float32).copy()\n",
    "    num_bbox = len(bboxes_coco)\n",
    "    labels = [0]*num_bbox\n",
    "  \n",
    "    label_path = train_df.iloc[row_idx].label_path[:62] + 'training_data' + train_df.iloc[row_idx].label_path[61:]\n",
    "    f = open(label_path, 'w')\n",
    "\n",
    "    if num_bbox < 1:\n",
    "        annot = ''\n",
    "        f.write(annot)\n",
    "        f.close()\n",
    "        cnt += 1\n",
    "        continue\n",
    "  \n",
    "    bboxes_yolo  = coco2yolo(image_height, image_width, bboxes_coco)\n",
    "\n",
    "    for i in range(len(bboxes_yolo)):\n",
    "        annot = [str(labels[i])] + list(bboxes_yolo[i].astype(str)) + (['\\n'] if num_bbox!=(i+1) else [''])\n",
    "        annot = ' '.join(annot)\n",
    "        annot = annot.strip(' ')\n",
    "        f.write(annot)\n",
    "    f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "36b0673c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3951, 13)\n",
      "(968, 13)\n"
     ]
    }
   ],
   "source": [
    "val_df = df[df['fold']==2]\n",
    "train_df = df[df['fold']!=2]\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d28cdada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Move labels from obj/ to test/ directory\n",
    "# def mv_labels (row):\n",
    "#     old_path = row.label_path\n",
    "#     filename = row.label_path.split('/')[-1]\n",
    "#     new_path = '/'.join(row.label_path.split('/')[:-2]) + '/test/' + filename\n",
    "#     row['label_path'] = new_path\n",
    "#     shutil.move(old_path, new_path)\n",
    "#     return row\n",
    "\n",
    "# val_df= val_df.apply(lambda x: mv_labels(x), axis=1)\n",
    "# val_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52b71f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       None\n",
       "1       None\n",
       "2       None\n",
       "3       None\n",
       "4       None\n",
       "        ... \n",
       "4914    None\n",
       "4915    None\n",
       "4916    None\n",
       "4917    None\n",
       "4918    None\n",
       "Length: 3951, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def copy_images (row):\n",
    "    old_path = row.image_path\n",
    "    new_path = row.image_path[:53] + 'datasets/training_data/images' + row.image_path[65:73] +'_'+row.image_path[74:]\n",
    "    shutil.copy(old_path, new_path)\n",
    "train_df.apply(lambda x: copy_images(x), axis=1)\n",
    "# train_df.apply(lambda x: copy_images(x), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55163ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
